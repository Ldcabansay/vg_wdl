# include the application.conf at the top
include required(classpath("application"))

system {
  job-rate-control {
    jobs = 1
    per = 60 second
  }
}

system.io {
  number-of-attempts = 5
}

backend {
  default = "Slurm"
  providers {
    Slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory" 
      config {
        concurrent-job-limit = 100
        script-epilogue = ""
        #call-caching {
        #  enabled = true
        #  invalidate-bad-cache-results = true
        #}
        runtime-attributes = """
        Int maxRetries = 5
        Int time = 600
        Int cpu = 2
        Float memory_gb = 20.0
        String? disks = "local-disk 20 SSD"
        String? docker
        """

        submit = """
            sbatch \
              --partition=norm \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${time} \
              ${"-c " + cpu} \
              --mem=$(echo $(printf %.0f "${memory_gb}")"g") \
              --gres=lscratch:$(echo "${disks}" | cut -f 2 -d " ") \
              --wrap "'/bin/bash ${script}'"
        """

        submit-docker = """
            if [ "${docker}" = "null" ]; then
                new_cwd=$(echo "${cwd}" | sed 's/\(\/cromwell-executions\).*/\1/g')
                new_script=$(echo "${script}" | sed 's/.*\(\/cromwell-executions\)/\1/g')
                sbatch \
                  --partition=norm \
                  -J ${job_name} \
                  -D ${cwd} \
                  -o ${cwd}/execution/stdout \
                  -e ${cwd}/execution/stderr \
                  -t ${time} \
                  ${"-c " + cpu} \
                  --mem=$(echo $(printf %.0f "${memory_gb}")"g") \
                  --gres=lscratch:$(echo "${disks}" | cut -f 2 -d " ") \
                  --wrap "'module load singularity; mkdir -p /lscratch/\$SLURM_JOB_ID/singularity_cache; export SINGULARITY_CACHEDIR=/lscratch/\$SLURM_JOB_ID/singularity_cache/; time singularity exec -H $new_cwd:/cromwell-executions --pwd ${docker_cwd} docker://ubuntu:latest ${job_shell} $new_script'"
            else
                # Submit the script to SLURM
                sbatch \
                  --partition=norm \
                  -J ${job_name} \
                  -D ${cwd} \
                  -o ${cwd}/execution/stdout \
                  -e ${cwd}/execution/stderr \
                  -t ${time} \
                  ${"-c " + cpu} \
                  --mem=$(echo $(printf %.0f "${memory_gb}")"g") \
                  --gres=lscratch:$(echo "${disks}" | cut -f 2 -d " ") \
                  --wrap "'module load singularity; mkdir -p /lscratch/\$SLURM_JOB_ID/singularity_cache; export SINGULARITY_CACHEDIR=/lscratch/\$SLURM_JOB_ID/singularity_cache/; time singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}'"
            fi
        """
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "(\\d+).*"
      }
    }
  }
}


