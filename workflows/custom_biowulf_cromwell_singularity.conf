# include the application.conf at the top
include required(classpath("application"))

backend {
  default = "Slurm"
  providers {
    Slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory" 
      config {
        concurrent-job-limit = 10
        system.io {
          number-of-attempts = 5
        }
        runtime-attributes = """
        Int maxRetries = 3
        Int time = 600
        Int cpu = 2
        Float memory_gb = 40.0
        String? disks = "local-disk 50 SSD"
        String? docker
        """

        submit = """
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${time} \
              ${"-c " + cpu} \
              --mem=$(echo $(printf %.0f "${memory_gb}")"g") \
              --gres=lscratch:$(echo "${disks}" | cut -f 2 -d " ") \
              --wrap "'/bin/bash ${script}'"
        """

        submit-docker = """
            # Submit the script to SLURM
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              -t ${time} \
              ${"-c " + cpu} \
              --mem=$(echo $(printf %.0f "${memory_gb}")"g") \
              --gres=lscratch:$(echo "${disks}" | cut -f 2 -d " ") \
              --wrap "'module load singularity; mkdir -p /lscratch/\$SLURM_JOB_ID/singularity_cache; export SINGULARITY_CACHEDIR=/lscratch/\$SLURM_JOB_ID/singularity_cache/; time singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}'"
        """
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "(\\d+).*"
      }
    }
  }
}


